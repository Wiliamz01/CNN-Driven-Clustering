{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# val_dir = './tiny-imagenet-200/val'\n",
    "# annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
    "\n",
    "# # Create a new directory for the restructured validation set\n",
    "# new_val_dir = os.path.join(val_dir, 'images_restructured')\n",
    "# os.makedirs(new_val_dir, exist_ok=True)\n",
    "\n",
    "# # Read the annotations file and move images to respective class folders\n",
    "# with open(annotations_file, 'r') as f:\n",
    "#     for line in f:\n",
    "#         parts = line.strip().split('\\t')\n",
    "#         img_name, img_class = parts[0], parts[1]\n",
    "        \n",
    "#         class_dir = os.path.join(new_val_dir, img_class)\n",
    "#         os.makedirs(class_dir, exist_ok=True)\n",
    "        \n",
    "#         src_path = os.path.join(val_dir, 'images', img_name)\n",
    "#         dst_path = os.path.join(class_dir, img_name)\n",
    "        \n",
    "#         try:\n",
    "#             shutil.move(src_path, dst_path)\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"File {src_path} not found. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_dir = './tiny-imagenet-200/train'\n",
    "val_dir = './tiny-imagenet-200/val/images_restructured'\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\William Zhang\\.conda\\envs\\search_sense\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Modify VGG16 for CIFAR-10\n",
    "model = vgg16(pretrained=True).to(device)  # Move model to GPU\n",
    "model.classifier[6] = nn.Linear(4096, 200).to(device)  # Replace last layer\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 5.364\n",
      "[1, 200] loss: 5.293\n",
      "[1, 300] loss: 5.253\n",
      "[2, 100] loss: 5.170\n",
      "[2, 200] loss: 5.091\n",
      "[2, 300] loss: 4.970\n",
      "[3, 100] loss: 4.457\n",
      "[3, 200] loss: 4.121\n",
      "[3, 300] loss: 3.819\n",
      "[4, 100] loss: 3.349\n",
      "[4, 200] loss: 3.201\n",
      "[4, 300] loss: 3.062\n",
      "[5, 100] loss: 2.850\n",
      "[5, 200] loss: 2.775\n",
      "[5, 300] loss: 2.658\n",
      "[6, 100] loss: 2.551\n",
      "[6, 200] loss: 2.511\n",
      "[6, 300] loss: 2.450\n",
      "[7, 100] loss: 2.349\n",
      "[7, 200] loss: 2.317\n",
      "[7, 300] loss: 2.300\n",
      "[8, 100] loss: 2.218\n",
      "[8, 200] loss: 2.202\n",
      "[8, 300] loss: 2.176\n",
      "[9, 100] loss: 2.119\n",
      "[9, 200] loss: 2.119\n",
      "[9, 300] loss: 2.091\n",
      "[10, 100] loss: 2.056\n",
      "[10, 200] loss: 2.038\n",
      "[10, 300] loss: 2.025\n",
      "[11, 100] loss: 1.975\n",
      "[11, 200] loss: 1.966\n",
      "[11, 300] loss: 1.973\n",
      "[12, 100] loss: 1.931\n",
      "[12, 200] loss: 1.914\n",
      "[12, 300] loss: 1.896\n",
      "[13, 100] loss: 1.895\n",
      "[13, 200] loss: 1.867\n",
      "[13, 300] loss: 1.863\n",
      "[14, 100] loss: 1.828\n",
      "[14, 200] loss: 1.834\n",
      "[14, 300] loss: 1.853\n",
      "[15, 100] loss: 1.802\n",
      "[15, 200] loss: 1.782\n",
      "[15, 300] loss: 1.796\n",
      "[16, 100] loss: 1.755\n",
      "[16, 200] loss: 1.775\n",
      "[16, 300] loss: 1.756\n",
      "[17, 100] loss: 1.741\n",
      "[17, 200] loss: 1.748\n",
      "[17, 300] loss: 1.741\n",
      "[18, 100] loss: 1.691\n",
      "[18, 200] loss: 1.689\n",
      "[18, 300] loss: 1.722\n",
      "[19, 100] loss: 1.706\n",
      "[19, 200] loss: 1.675\n",
      "[19, 300] loss: 1.665\n",
      "[20, 100] loss: 1.654\n",
      "[20, 200] loss: 1.639\n",
      "[20, 300] loss: 1.660\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print average loss every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Step the scheduler based on the validation loss\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    scheduler.step(val_loss / len(testloader))\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8823, 25088)\n"
     ]
    }
   ],
   "source": [
    "def extract_batch_features(inputs, model):\n",
    "    with torch.no_grad():\n",
    "        features = model.features(inputs)\n",
    "    features = features.view(features.size(0), -1)\n",
    "    return features.cpu()  # Move features back to CPU for further processing\n",
    "\n",
    "# Extract features for the entire CIFAR-10 test dataset\n",
    "feature_list = []\n",
    "\n",
    "for inputs, _ in testloader:\n",
    "    inputs = inputs.to(device)\n",
    "    features = extract_batch_features(inputs, model)\n",
    "    feature_list.append(features)\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "feature_tensor = torch.cat(feature_list, dim=0)\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "feature_array = feature_tensor.numpy()\n",
    "\n",
    "print(feature_array.shape)  # This should print (10000, [feature_dimension])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_sense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
